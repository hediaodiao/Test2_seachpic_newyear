#!/usr/bin/env python3
"""
å›¾åƒç‰¹å¾æå–ä¸ç›¸ä¼¼åº¦åˆ†æ - ä¼˜åŒ–ç‰ˆ
æ”¯æŒå¤šç§æ¨¡å‹é€‰æ‹©ï¼Œæœ¬åœ°ç¼“å­˜ç®¡ç†
"""

import os
import sys
import ssl
import torch
import torch.nn as nn
from torchvision import models, transforms
from PIL import Image
import numpy as np
# import matplotlib.pyplot as plt
# from sklearn.manifold import TSNE
# from sklearn.decomposition import PCA
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# ========== 1. SSLè¯ä¹¦ä¿®å¤ ==========
ssl._create_default_https_context = ssl._create_unverified_context

# ========== 2. æœ¬åœ°ç¼“å­˜ç®¡ç†å™¨ ==========
class ModelCacheManager:
    """æ¨¡å‹ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir="./model_cache"):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        
        å‚æ•°:
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®PyTorchç¼“å­˜ç¯å¢ƒå˜é‡
        os.environ['TORCH_HOME'] = str(self.cache_dir)
        
        # æ¨¡å‹æ–‡ä»¶æ˜ å°„
        self.model_files = {
            'resnet50': 'resnet50-11ad3fa6.pth',
            'efficientnet_lite0': 'efficientnet_lite0-0aa5c2b1.pth',
            'mobilenet_v3_small': 'mobilenet_v3_small-047dcff4.pth',
            'convnext_tiny': 'convnext_tiny-983f1584.pth',
            'openclip_vit_b_32': 'open_clip_model.safetensors',
            'openclip_vit_l_14': 'open_clip_model_vit_l_14.safetensors',
            'dinov2_vit_s': 'dinov2_vit_small.pth',
        }
        
        # æ¨¡å‹åŠ è½½å‡½æ•°æ˜ å°„
        self.model_loaders = {
            'resnet50': models.resnet50,
            'efficientnet_lite0': self._load_efficientnet_lite0,
            'mobilenet_v3_small': models.mobilenet_v3_small,
            'convnext_tiny': models.convnext_tiny,
            'openclip_vit_b_32': self._load_openclip_vit_b_32,
            'openclip_vit_l_14': self._load_openclip_vit_l_14,
            'dinov2_vit_s': self._load_dinov2_vit_s,
        }
        
        print(f"ğŸ“ æ¨¡å‹ç¼“å­˜ç›®å½•: {self.cache_dir.absolute()}")
    
    def get_model_path(self, model_name):
        """è·å–æ¨¡å‹æ–‡ä»¶è·¯å¾„"""
        if model_name not in self.model_files:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
        return self.cache_dir / self.model_files[model_name]
    
    def is_model_cached(self, model_name):
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç¼“å­˜"""
        model_path = self.get_model_path(model_name)
        return model_path.exists()
    
    def load_model_from_cache(self, model_name, weights='IMAGENET1K_V1'):
        """
        ä»ç¼“å­˜åŠ è½½æ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¸‹è½½
        
        å‚æ•°:
            model_name: æ¨¡å‹åç§°
            weights: æƒé‡ç±»å‹
            
        è¿”å›:
            model: åŠ è½½çš„æ¨¡å‹
        """
        if model_name not in self.model_loaders:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
        
        # æ£€æŸ¥ç¼“å­˜
        if self.is_model_cached(model_name):
            print(f"âœ… ä»ç¼“å­˜åŠ è½½: {model_name}")
            try:
                # å°è¯•ä»ç¼“å­˜åŠ è½½
                return self.model_loaders[model_name](weights=weights)
            except:
                # å¦‚æœç¼“å­˜æ–‡ä»¶æŸåï¼Œåˆ é™¤å¹¶é‡æ–°ä¸‹è½½
                model_path = self.get_model_path(model_name)
                print(f"âš  ç¼“å­˜æ–‡ä»¶æŸåï¼Œåˆ é™¤: {model_path}")
                model_path.unlink(missing_ok=True)
        
        # ä¸‹è½½æ¨¡å‹
        print(f"ğŸ“¥ ä¸‹è½½æ¨¡å‹: {model_name}")
        try:
            model = self.model_loaders[model_name](weights=weights)
            print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {model_name}")
            return model
        except Exception as e:
            print(f"âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
            raise
    
    def _load_efficientnet_lite0(self, weights=None):
        """
        åŠ è½½EfficientNet-B0æ¨¡å‹ï¼ˆæ›¿ä»£EfficientNet-Lite0ï¼‰
        
        å‚æ•°:
            weights: æƒé‡å‚æ•°ï¼ˆä¸ä½¿ç”¨ï¼‰
            
        è¿”å›:
            model: EfficientNet-B0æ¨¡å‹
        """
        try:
            from efficientnet_pytorch import EfficientNet
            model = EfficientNet.from_pretrained('efficientnet-b0')
            return model
        except ImportError:
            print("âŒ éœ€è¦å®‰è£… efficientnet-pytorch åŒ…")
            print("è¯·è¿è¡Œ: pip install efficientnet-pytorch")
            raise
    
    def _load_openclip_vit_b_32(self, weights=None):
        """
        åŠ è½½OpenCLIP ViT-B/32æ¨¡å‹
        
        å‚æ•°:
            weights: æƒé‡å‚æ•°ï¼ˆä¸ä½¿ç”¨ï¼‰
            
        è¿”å›:
            model: OpenCLIPæ¨¡å‹
        """
        try:
            import open_clip
            import os
            from pathlib import Path
            
            # è®¾ç½®ç¦»çº¿æ¨¡å¼ç¯å¢ƒå˜é‡
            os.environ['HF_HUB_OFFLINE'] = '1'
            os.environ['TRANSFORMERS_OFFLINE'] = '1'
            
            # æ„å»ºæœ¬åœ°æ¨¡å‹æ–‡ä»¶è·¯å¾„
            local_model_path = Path(self.cache_dir) / "timm" / "vit_base_patch32_clip_224.laion2b_e16" / "open_clip_model.safetensors"
            
            if local_model_path.exists():
                # å¦‚æœæœ¬åœ°æœ‰æ¨¡å‹æ–‡ä»¶ï¼Œç›´æ¥åŠ è½½
                model, _, preprocess = open_clip.create_model_and_transforms(
                    'ViT-B-32', 
                    pretrained=str(local_model_path)
                )
                print(f"âœ… ä»æœ¬åœ°æ–‡ä»¶åŠ è½½ OpenCLIP æ¨¡å‹: {local_model_path}")
            else:
                # å¦åˆ™ä»ç½‘ç»œä¸‹è½½ï¼ˆè¿™å¯èƒ½ä¼šå¤±è´¥ï¼Œå¦‚æœæ²¡æœ‰ç½‘ç»œè¿æ¥ï¼‰
                print("âš ï¸ æœ¬åœ°æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä»ç½‘ç»œåŠ è½½...")
                model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_e16')
            
            # æ›´æ–°å®ä¾‹çš„é¢„å¤„ç†æ–¹æ³•ï¼Œå› ä¸ºCLIPæœ‰è‡ªå·±çš„é¢„å¤„ç†
            self.clip_preprocess = preprocess
            return model
        except ImportError:
            print("âŒ éœ€è¦å®‰è£… open_clip åŒ…")
            print("è¯·è¿è¡Œ: pip install open_clip_torch")
            raise
        except Exception as e:
            print(f"âŒ OpenCLIPæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("è¯·ç¡®ä¿å·²å®‰è£… open_clip_torch åŒ…å¹¶ç¡®ä¿æ¨¡å‹æ–‡ä»¶å­˜åœ¨")
            raise
    
    def _load_openclip_vit_l_14(self, weights=None):
        """
        åŠ è½½OpenCLIP ViT-L/14æ¨¡å‹å¹¶è½¬æ¢ä¸ºINT8ç²¾åº¦
        
        å‚æ•°:
            weights: æƒé‡å‚æ•°ï¼ˆä¸ä½¿ç”¨ï¼‰
            
        è¿”å›:
            model: OpenCLIPæ¨¡å‹ï¼ˆINT8ç²¾åº¦ï¼‰
        """
        try:
            import open_clip
            import os
            import torch
            from pathlib import Path
            
            # æ„å»ºæœ¬åœ°æ¨¡å‹æ–‡ä»¶è·¯å¾„
            # 1. æ£€æŸ¥model_cacheç›®å½•ä¸­ç°æœ‰çš„OpenCLIPæ¨¡å‹æ–‡ä»¶
            openclip_model_paths = [
                Path(self.cache_dir) / "open_clip_pytorch_model.safetensors",
                Path(self.cache_dir) / "open_clip_pytorch_model.bin",
                Path(self.cache_dir) / "model.safetensors",
                Path(self.cache_dir) / "pytorch_model.bin",
                Path(self.cache_dir) / "open_clip_model_vit_l_14.safetensors"
            ]
            
            # 2. æ£€æŸ¥ç”¨æˆ·æä¾›çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„
            user_model_path = Path(self.cache_dir) / "vit_l_14-laion2b_s32b_b82k.bin"
            
            # 3. åŒæ—¶æ£€æŸ¥å­ç›®å½•ä¸­çš„æ¨¡å‹æ–‡ä»¶ï¼ˆå…¼å®¹æ—§è·¯å¾„ï¼‰
            local_model_dir = Path(self.cache_dir) / "timm" / "vit_large_patch14_clip_224.laion2b_e16"
            local_model_path_subdir = local_model_dir / "open_clip_model_vit_l_14.safetensors"
            
            # ç¡®ä¿æœ¬åœ°æ¨¡å‹ç›®å½•å­˜åœ¨
            local_model_dir.mkdir(parents=True, exist_ok=True)
            
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆæŒ‰ä¼˜å…ˆçº§é¡ºåºï¼‰
            model_path_to_use = None
            # æ£€æŸ¥ç”¨æˆ·æä¾›çš„è·¯å¾„
            if user_model_path.exists():
                model_path_to_use = user_model_path
                print(f"âœ… æ‰¾åˆ°ç”¨æˆ·æä¾›çš„æ¨¡å‹æ–‡ä»¶: {model_path_to_use}")
            # æ£€æŸ¥model_cacheç›®å½•ä¸­çš„OpenCLIPæ¨¡å‹æ–‡ä»¶
            else:
                for path in openclip_model_paths:
                    if path.exists():
                        model_path_to_use = path
                        print(f"âœ… æ‰¾åˆ°æœ¬åœ°OpenCLIPæ¨¡å‹æ–‡ä»¶: {model_path_to_use}")
                        break
            # æ£€æŸ¥å­ç›®å½•ä¸­çš„æ¨¡å‹æ–‡ä»¶
            if not model_path_to_use and local_model_path_subdir.exists():
                model_path_to_use = local_model_path_subdir
                print(f"âœ… æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_path_to_use}")
            
            if model_path_to_use:
                # å¦‚æœæœ¬åœ°æœ‰æ¨¡å‹æ–‡ä»¶ï¼Œç›´æ¥åŠ è½½
                try:
                    # è®¾ç½®ç¦»çº¿æ¨¡å¼ç¯å¢ƒå˜é‡
                    os.environ['HF_HUB_OFFLINE'] = '1'
                    os.environ['TRANSFORMERS_OFFLINE'] = '1'
                    
                    model, _, preprocess = open_clip.create_model_and_transforms(
                        'ViT-L-14', 
                        pretrained=str(model_path_to_use)
                    )
                    print(f"âœ… ä»æœ¬åœ°æ–‡ä»¶åŠ è½½ OpenCLIP ViT-L/14 æ¨¡å‹: {model_path_to_use}")
                except Exception as e:
                    print(f"âš ï¸ ä»æœ¬åœ°æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
                    print("âš ï¸ å°è¯•ä½¿ç”¨é»˜è®¤çš„CLIPæ¨¡å‹...")
                    # å°è¯•ä½¿ç”¨é»˜è®¤çš„CLIPæ¨¡å‹ï¼Œä¸æŒ‡å®špretrained
                    model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14')
            else:
                # å¦åˆ™ä»ç½‘ç»œä¸‹è½½ï¼ˆè¿™å¯èƒ½ä¼šå¤±è´¥ï¼Œå¦‚æœæ²¡æœ‰ç½‘ç»œè¿æ¥ï¼‰
                print("âš ï¸ æœ¬åœ°æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä»ç½‘ç»œåŠ è½½...")
                try:
                    # å°è¯•ä½¿ç”¨laion2b_s32b_b82kæƒé‡
                    model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='laion2b_s32b_b82k')
                except Exception as e:
                    print(f"âš ï¸ ä»ç½‘ç»œåŠ è½½å¤±è´¥: {e}")
                    print("âš ï¸ å°è¯•ä½¿ç”¨é»˜è®¤çš„CLIPæ¨¡å‹...")
                    # å°è¯•ä½¿ç”¨é»˜è®¤çš„CLIPæ¨¡å‹ï¼Œä¸æŒ‡å®špretrained
                    model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14')
            
            # å°†æ¨¡å‹è½¬æ¢ä¸ºINT8ç²¾åº¦
            print("ğŸ”„ æ­£åœ¨å°†æ¨¡å‹è½¬æ¢ä¸ºINT8ç²¾åº¦...")
            try:
                # ä½¿ç”¨åŠ¨æ€é‡åŒ–ï¼Œé€‚ç”¨äºCPUæ¨ç†
                model_int8 = torch.quantization.quantize_dynamic(
                    model,  # è¦é‡åŒ–çš„æ¨¡å‹
                    {torch.nn.Linear, torch.nn.Conv2d},  # è¦é‡åŒ–çš„å±‚ç±»å‹
                    dtype=torch.qint8  # é‡åŒ–ç›®æ ‡ç±»å‹
                )
                print("âœ… æ¨¡å‹å·²æˆåŠŸè½¬æ¢ä¸ºINT8ç²¾åº¦")
            except Exception as e:
                print(f"âš ï¸ æ¨¡å‹é‡åŒ–å¤±è´¥: {e}")
                print("âš ï¸ ä½¿ç”¨åŸå§‹ç²¾åº¦æ¨¡å‹")
                model_int8 = model
            
            # æ›´æ–°å®ä¾‹çš„é¢„å¤„ç†æ–¹æ³•ï¼Œå› ä¸ºCLIPæœ‰è‡ªå·±çš„é¢„å¤„ç†
            self.clip_preprocess = preprocess
            return model_int8
        except ImportError:
            print("âŒ éœ€è¦å®‰è£… open_clip åŒ…")
            print("è¯·è¿è¡Œ: pip install open_clip_torch")
            raise
        except Exception as e:
            print(f"âŒ OpenCLIP ViT-L/14 æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("è¯·ç¡®ä¿å·²å®‰è£… open_clip_torch åŒ…å¹¶ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸")
            print("\nğŸ“ æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶åˆ°ä»¥ä¸‹ç›®å½•:")
            print(f"{Path(self.cache_dir) / 'timm' / 'vit_large_patch14_clip_224.laion2b_e16'}")
            print("\nğŸ”— æ¨¡å‹ä¸‹è½½é“¾æ¥:")
            print("https://huggingface.co/laion/CLIP-ViT-L-14-laion2B-s32B-b82K/resolve/main/open_clip_model.safetensors")
            print("\nğŸ“„ ä¸‹è½½åé‡å‘½åä¸º:")
            print("open_clip_model_vit_l_14.safetensors")
            raise
    
    def _load_dinov2_vit_s(self, weights=None):
        """
        åŠ è½½DINOv2 ViT-Sæ¨¡å‹
        
        å‚æ•°:
            weights: æƒé‡å‚æ•°ï¼ˆä¸ä½¿ç”¨ï¼‰
            
        è¿”å›:
            model: DINOv2 ViT-Sæ¨¡å‹
        """
        try:
            import torch
            import torchvision.models as models
            
            # æ£€æŸ¥æ˜¯å¦å®‰è£…äº†torchvision>=0.16.0ï¼ˆæ”¯æŒDINOv2ï¼‰
            from torchvision import __version__
            version = tuple(map(int, __version__.split('.')[:2]))
            if version < (0, 16):
                print("âš ï¸ torchvisionç‰ˆæœ¬è¿‡ä½ï¼Œå¯èƒ½ä¸æ”¯æŒDINOv2")
                print("å»ºè®®å®‰è£… torchvision>=0.16.0")
            
            # å°è¯•åŠ è½½DINOv2 ViT-Sæ¨¡å‹
            # æ³¨æ„ï¼šDINOv2çš„æ­£ç¡®æ¨¡å‹åç§°æ˜¯'dinov2_vits14'ï¼Œå…¶ä¸­'vits'è¡¨ç¤ºViT-Smallï¼Œ'14'è¡¨ç¤ºpatch size
            model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')
            print("âœ… DINOv2 ViT-Sæ¨¡å‹åŠ è½½æˆåŠŸ")
            return model
        except ImportError as e:
            print("âŒ å¯¼å…¥é”™è¯¯ï¼Œå¯èƒ½éœ€è¦å®‰è£…ç›¸å…³ä¾èµ–")
            print(f"é”™è¯¯: {e}")
            print("è¯·ç¡®ä¿å·²å®‰è£… torch å’Œ torchvision")
            raise
        except Exception as e:
            print(f"âŒ DINOv2æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("è¯·ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸ï¼Œæˆ–å·²åœ¨æœ¬åœ°ç¼“å­˜æ¨¡å‹")
            raise
    
    def get_cached_models(self):
        """è·å–å·²ç¼“å­˜çš„æ¨¡å‹åˆ—è¡¨"""
        cached_models = []
        for model_name, filename in self.model_files.items():
            if (self.cache_dir / filename).exists():
                cached_models.append(model_name)
        return cached_models
    
    def clear_cache(self, model_name=None):
        """æ¸…ç†ç¼“å­˜"""
        if model_name:
            # æ¸…ç†æŒ‡å®šæ¨¡å‹
            model_path = self.get_model_path(model_name)
            if model_path.exists():
                model_path.unlink()
                print(f"ğŸ—‘ å·²åˆ é™¤ç¼“å­˜: {model_name}")
        else:
            # æ¸…ç†æ‰€æœ‰ç¼“å­˜
            for file in self.cache_dir.glob("*.pth"):
                file.unlink()
            print(f"ğŸ—‘ å·²æ¸…ç†æ‰€æœ‰ç¼“å­˜")

# ========== 3. é€šç”¨ç‰¹å¾æå–å™¨ ==========
class FeatureExtractor:
    """é€šç”¨ç‰¹å¾æå–å™¨ï¼Œæ”¯æŒå¤šç§æ¨¡å‹"""
    
    SUPPORTED_MODELS = ['resnet50', 'efficientnet_lite0', 'mobilenet_v3_small', 'convnext_tiny', 'openclip_vit_b_32', 'openclip_vit_l_14', 'dinov2_vit_s']
    
    def __init__(self, model_name='resnet50', device='auto', cache_dir="./model_cache"):
        """
        åˆå§‹åŒ–ç‰¹å¾æå–å™¨
        
        å‚æ•°:
            model_name: æ¨¡å‹åç§°
            device: è®¾å¤‡ç±»å‹
            cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•
        """
        if model_name not in self.SUPPORTED_MODELS:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}ã€‚æ”¯æŒ: {self.SUPPORTED_MODELS}")
        
        self.model_name = model_name
        self.device = self._get_device(device)
        self.cache_manager = ModelCacheManager(cache_dir)
        self.model = None
        self.preprocess = None
        
        # æ·»åŠ çº¿ç¨‹é”ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹çš„å®‰å…¨æ€§
        import threading
        self.model_lock = threading.Lock()
        
        self._initialize_model()
        self._initialize_preprocess()
    
    def _get_device(self, device):
        """è·å–å¯ç”¨çš„è®¾å¤‡"""
        if device == 'auto':
            if torch.backends.mps.is_available():
                return torch.device("mps")
            elif torch.cuda.is_available():
                return torch.device("cuda")
            else:
                return torch.device("cpu")
        elif device == 'mps' and torch.backends.mps.is_available():
            return torch.device("mps")
        elif device == 'cuda' and torch.cuda.is_available():
            return torch.device("cuda")
        else:
            return torch.device("cpu")
    
    def _initialize_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        print(f"æ­£åœ¨åŠ è½½ {self.model_name} æ¨¡å‹...")
        
        # ä»ç¼“å­˜åŠ è½½æˆ–ä¸‹è½½æ¨¡å‹
        model = self.cache_manager.load_model_from_cache(self.model_name)
        
        # ä¿®æ”¹æ¨¡å‹ç»“æ„ï¼Œæå–ç‰¹å¾
        if self.model_name == 'resnet50':
            # ResNet50ï¼šç§»é™¤æœ€åçš„å…¨è¿æ¥å±‚
            self.model = nn.Sequential(*list(model.children())[:-1])
            self.feature_dim = 2048
            
        elif self.model_name == 'efficientnet_lite0':
            # EfficientNet-Lite0ï¼šç§»é™¤æœ€åçš„åˆ†ç±»å±‚
            # EfficientNet æ¨¡å‹æœ‰ _fc å±‚ä½œä¸ºåˆ†ç±»å™¨
            if hasattr(model, '_fc'):
                model._fc = nn.Identity()
            elif hasattr(model, 'classifier'):
                model.classifier = nn.Identity()
            elif hasattr(model, 'fc'):
                model.fc = nn.Identity()
            else:
                # å¦‚æœä»¥ä¸Šéƒ½æ²¡æœ‰ï¼Œå°è¯•ç§»é™¤æœ€åä¸€å±‚
                model = nn.Sequential(*list(model.children())[:-1])
            
            self.model = model
            self.feature_dim = 1280
                
        elif self.model_name == 'mobilenet_v3_small':
            # MobileNetV3-Smallï¼šç§»é™¤æœ€åçš„åˆ†ç±»å±‚
            if hasattr(model, 'classifier'):
                # MobileNetV3 çš„åˆ†ç±»å™¨é€šå¸¸æ˜¯ [GlobalAveragePool, Dropout, Linear]
                features = list(model.children())[:-1]  # ç§»é™¤ classifier
                # æ·»åŠ å…¨å±€å¹³å‡æ± åŒ–å±‚ä»¥è·å¾—å›ºå®šå¤§å°çš„ç‰¹å¾
                features.append(nn.AdaptiveAvgPool2d((1, 1)))
                self.model = nn.Sequential(*features)
                self.feature_dim = 576  # MobileNetV3-Small çš„ç‰¹å¾ç»´åº¦
            else:
                # å¦‚æœæ²¡æœ‰classifierï¼Œç›´æ¥ç§»é™¤æœ€åå‡ å±‚
                self.model = nn.Sequential(*list(model.children())[:-1])
                self.feature_dim = 576
                
        elif self.model_name == 'convnext_tiny':
            # ConvNeXt-Tinyï¼šç§»é™¤æœ€åçš„åˆ†ç±»å±‚
            if hasattr(model, 'classifier'):
                # ConvNeXt çš„åˆ†ç±»å™¨é€šå¸¸æ˜¯ LayerNorm + AdaptiveAvgPool + Linear
                features = list(model.children())[:-1]  # ç§»é™¤ classifier
                self.model = nn.Sequential(*features)
                self.feature_dim = 768  # ConvNeXt-Tiny çš„ç‰¹å¾ç»´åº¦
            else:
                self.model = nn.Sequential(*list(model.children())[:-1])
                self.feature_dim = 768
                
        elif self.model_name == 'openclip_vit_b_32':
            # OpenCLIP ViT-B/32ï¼šç§»é™¤æœ€åçš„æŠ•å½±å±‚
            # OpenCLIP æ¨¡å‹ç»“æ„ä¸åŒï¼Œé€šå¸¸æœ‰transformerå’Œprojectionä¸¤éƒ¨åˆ†
            if hasattr(model, 'visual'):
                # ä½¿ç”¨è§†è§‰ç¼–ç å™¨éƒ¨åˆ†
                visual_model = model.visual
                # ä¿ç•™transformeréƒ¨åˆ†ï¼Œä½†ç§»é™¤æœ€ç»ˆçš„æŠ•å½±å±‚
                self.model = visual_model
                self.feature_dim = 512  # ViT-B/32 è§†è§‰ç¼–ç å™¨çš„ç‰¹å¾ç»´åº¦
            else:
                # å¦‚æœæ²¡æœ‰å•ç‹¬çš„visualç»„ä»¶ï¼Œåˆ™ä½¿ç”¨æ•´ä¸ªæ¨¡å‹
                self.model = model
                self.feature_dim = 512
        elif self.model_name == 'openclip_vit_l_14':
            # OpenCLIP ViT-L/14ï¼šç§»é™¤æœ€åçš„æŠ•å½±å±‚
            # OpenCLIP æ¨¡å‹ç»“æ„ä¸åŒï¼Œé€šå¸¸æœ‰transformerå’Œprojectionä¸¤éƒ¨åˆ†
            # æ³¨æ„ï¼šé‡åŒ–åçš„æ¨¡å‹å¯èƒ½æ²¡æœ‰visualå±æ€§ï¼Œç›´æ¥ä½¿ç”¨æ•´ä¸ªæ¨¡å‹
            try:
                if hasattr(model, 'visual'):
                    # ä½¿ç”¨è§†è§‰ç¼–ç å™¨éƒ¨åˆ†
                    visual_model = model.visual
                    # ä¿ç•™transformeréƒ¨åˆ†ï¼Œä½†ç§»é™¤æœ€ç»ˆçš„æŠ•å½±å±‚
                    self.model = visual_model
                    self.feature_dim = 768  # ViT-L/14 è§†è§‰ç¼–ç å™¨çš„ç‰¹å¾ç»´åº¦
                else:
                    # å¦‚æœæ²¡æœ‰å•ç‹¬çš„visualç»„ä»¶ï¼Œåˆ™ä½¿ç”¨æ•´ä¸ªæ¨¡å‹
                    self.model = model
                    self.feature_dim = 768
            except Exception as e:
                print(f"âš ï¸ å¤„ç†openclip_vit_l_14æ¨¡å‹æ—¶å‡ºé”™: {e}")
                print("âš ï¸ ç›´æ¥ä½¿ç”¨æ•´ä¸ªæ¨¡å‹")
                # ç›´æ¥ä½¿ç”¨æ•´ä¸ªæ¨¡å‹
                self.model = model
                self.feature_dim = 768
        
        elif self.model_name == 'dinov2_vit_s':
            # DINOv2 ViT-Sï¼šä½¿ç”¨æ•´ä¸ªæ¨¡å‹ï¼Œæå–CLS tokençš„ç‰¹å¾
            # DINOv2æ¨¡å‹è¾“å‡ºåŒ…å«å¤šä¸ªéƒ¨åˆ†ï¼Œæˆ‘ä»¬ä½¿ç”¨æœ€åä¸€å±‚çš„ç‰¹å¾
            self.model = model
            self.feature_dim = 384  # ViT-S çš„ç‰¹å¾ç»´åº¦
                
        # å°†æ¨¡å‹ç§»åˆ°è®¾å¤‡
        # æ³¨æ„ï¼šé‡åŒ–åçš„æ¨¡å‹åªèƒ½åœ¨CPUä¸Šè¿è¡Œ
        if self.model_name == 'openclip_vit_l_14':
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦è¢«é‡åŒ–
            is_quantized = any(hasattr(module, 'qconfig') for module in self.model.modules())
            if is_quantized:
                print("âš ï¸ é‡åŒ–æ¨¡å‹åªèƒ½åœ¨CPUä¸Šè¿è¡Œï¼Œå°†è®¾å¤‡è®¾ç½®ä¸ºCPU")
                self.device = torch.device("cpu")
        
        self.model = self.model.to(self.device)
        self.model.eval()
        
        print(f"âœ“ {self.model_name} åŠ è½½å®Œæˆ")
        print(f"  è®¾å¤‡: {self.device}")
        print(f"  ç‰¹å¾ç»´åº¦: {self.feature_dim}")
    
    def _initialize_preprocess(self):
        """åˆå§‹åŒ–é¢„å¤„ç†ç®¡é“"""
        # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©é¢„å¤„ç†å‚æ•°
        if self.model_name in ['resnet50', 'efficientnet_lite0', 'mobilenet_v3_small']:
            # ä½¿ç”¨ImageNeté¢„å¤„ç†å‚æ•°
            self.preprocess = transforms.Compose([
                transforms.Resize(256),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
                transforms.Normalize(
                    mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225]
                ),
            ])
        elif self.model_name == 'convnext_tiny':
            # ConvNeXtæ¨¡å‹ä½¿ç”¨ç›¸åŒçš„é¢„å¤„ç†å‚æ•°
            self.preprocess = transforms.Compose([
                transforms.Resize(256),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
                transforms.Normalize(
                    mean=[0.485, 0.456, 0.456],
                    std=[0.229, 0.224, 0.225]
                ),
            ])
        elif self.model_name in ['openclip_vit_b_32', 'openclip_vit_l_14']:
            # OpenCLIPä½¿ç”¨ç‰¹å®šçš„é¢„å¤„ç†ï¼Œå·²åœ¨æ¨¡å‹åŠ è½½æ—¶è®¾ç½®
            # å¦‚æœæ²¡æœ‰é¢„å¤„ç†å‡½æ•°ï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„
            if hasattr(self.cache_manager, 'clip_preprocess'):
                self.preprocess = self.cache_manager.clip_preprocess
            else:
                # é»˜è®¤é¢„å¤„ç†
                self.preprocess = transforms.Compose([
                    transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),
                    transforms.CenterCrop(224),
                    transforms.ToTensor(),
                    transforms.Normalize(
                        mean=[0.48145466, 0.4578275, 0.40821073],
                        std=[0.26862954, 0.26130258, 0.27577711]
                    ),
                ])
        
        elif self.model_name == 'dinov2_vit_s':
            # DINOv2ä½¿ç”¨ç‰¹å®šçš„é¢„å¤„ç†å‚æ•°
            self.preprocess = transforms.Compose([
                transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
                transforms.Normalize(
                    mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225]
                ),
            ])
    
    # ========== å¯¹æ¯”å­¦ä¹ ç›¸å…³ä»£ç å·²æ³¨é‡Š ==========
    # def _initialize_contrastive_augmentation(self):
    #     """åˆå§‹åŒ–å¯¹æ¯”å­¦ä¹ çš„æ•°æ®å¢å¼ºç®¡é“"""
    #     self.contrastive_augmentation = transforms.Compose([
    #         transforms.RandomResizedCrop(224),
    #         transforms.RandomHorizontalFlip(p=0.5),
    #         transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),
    #         transforms.RandomGrayscale(p=0.2),
    #         transforms.ToTensor(),
    #         transforms.Normalize(
    #             mean=[0.485, 0.456, 0.406],
    #             std=[0.229, 0.224, 0.225]
    #         ),
    #     ])
    
    def extract_features(self, image_path):
        """
        ä»å•å¼ å›¾ç‰‡æå–ç‰¹å¾
        
        å‚æ•°:
            image_path: å›¾ç‰‡è·¯å¾„
            
        è¿”å›:
            features: ç‰¹å¾å‘é‡
        """
        if not os.path.exists(image_path):
            raise FileNotFoundError(f"å›¾ç‰‡ä¸å­˜åœ¨: {image_path}")
        
        # åŠ è½½å›¾ç‰‡
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            raise ValueError(f"æ— æ³•åŠ è½½å›¾ç‰‡: {e}")
        
        # é¢„å¤„ç†
        input_tensor = self.preprocess(image)
        input_batch = input_tensor.unsqueeze(0).to(self.device)
        
        # æå–ç‰¹å¾
        with torch.no_grad():
            features = self.model(input_batch)
            
            # æ ¹æ®æ¨¡å‹ç±»å‹å¤„ç†è¾“å‡º
            if self.model_name == 'resnet50':
                features = features.squeeze()  # [1, 2048, 1, 1] -> [2048]
            elif self.model_name == 'efficientnet_lite0':
                # EfficientNet ç§»é™¤ _fc å±‚åï¼Œè¾“å‡ºå·²ç»æ˜¯ [batch_size, 1280]
                if len(features.shape) == 4:
                    features = features.mean([2, 3]).squeeze()  # å…¨å±€å¹³å‡æ± åŒ–
                else:
                    features = features.squeeze()
            elif self.model_name == 'mobilenet_v3_small':
                # MobileNetV3-Small ç»è¿‡è‡ªå®šä¹‰ç»“æ„åï¼Œè¾“å‡º [batch_size, 576, 1, 1]
                features = features.squeeze()  # [1, 576, 1, 1] -> [576]
            elif self.model_name == 'convnext_tiny':
                # ConvNeXt-Tiny è¾“å‡º [batch_size, 768, 1, 1]ï¼Œéœ€è¦å…¨å±€å¹³å‡æ± åŒ–
                if len(features.shape) == 4:
                    features = features.mean([2, 3]).squeeze()  # å…¨å±€å¹³å‡æ± åŒ–
                else:
                    features = features.squeeze()
            elif self.model_name in ['openclip_vit_b_32', 'openclip_vit_l_14']:
                # OpenCLIP æ¨¡å‹è¾“å‡ºç‰¹å¾å‘é‡
                if len(features.shape) == 4:
                    features = features.mean([2, 3]).squeeze()  # å…¨å±€å¹³å‡æ± åŒ–
                else:
                    features = features.squeeze()
            elif self.model_name == 'dinov2_vit_s':
                # DINOv2 ViT-S è¾“å‡ºåŒ…å«CLS tokenå’Œpatch tokens
                # æˆ‘ä»¬ä½¿ç”¨CLS tokençš„ç‰¹å¾ï¼Œå®ƒæ˜¯è¾“å‡ºçš„ç¬¬ä¸€ä¸ªå…ƒç´ 
                if isinstance(features, dict):
                    # å¦‚æœè¾“å‡ºæ˜¯å­—å…¸ï¼Œè·å–æœ€åä¸€å±‚çš„ç‰¹å¾
                    if 'last_hidden_state' in features:
                        features = features['last_hidden_state'][:, 0].squeeze()  # è·å–CLS token
                    else:
                        # å°è¯•è·å–å…¶ä»–å¯èƒ½çš„ç‰¹å¾é”®
                        for key in features:
                            if isinstance(features[key], torch.Tensor):
                                features = features[key]
                                if len(features.shape) > 1:
                                    features = features[:, 0].squeeze()
                                break
                elif len(features.shape) == 3:
                    # å¦‚æœè¾“å‡ºæ˜¯ [batch_size, seq_len, hidden_dim]ï¼Œè·å–CLS token
                    features = features[:, 0].squeeze()  # è·å–ç¬¬ä¸€ä¸ªtoken (CLS)
                else:
                    features = features.squeeze()
        
        features = features.cpu().numpy()
        
        return features
    
    def extract_batch_features(self, image_paths, show_progress=True, base_dir=None, batch_size=32):
        """
        æ‰¹é‡æå–ç‰¹å¾
        
        å‚æ•°:
            image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            base_dir: åŸºç¡€ç›®å½•è·¯å¾„ï¼Œç”¨äºè®¡ç®—ç›¸å¯¹è·¯å¾„
            batch_size: æ‰¹é‡å¤§å°
            
        è¿”å›:
            features_dict: å­—å…¸ {ç›¸å¯¹è·¯å¾„: ç‰¹å¾å‘é‡}
        """
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import os
        
        features_dict = {}
        total_images = len(image_paths)
        
        # ç”Ÿæˆæ‰€æœ‰æ‰¹æ¬¡çš„ç´¢å¼•
        batch_indices = [(i, min(i + batch_size, total_images)) for i in range(0, total_images, batch_size)]
        
        # è·å–CPUæ ¸å¿ƒæ•°ï¼Œè®¾ç½®çº¿ç¨‹æ± å¤§å°
        max_workers = min(8, os.cpu_count() or 4)
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†æ‰¹æ¬¡
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰æ‰¹æ¬¡å¤„ç†ä»»åŠ¡
            future_to_batch = {}
            for batch_start, batch_end in batch_indices:
                batch_paths = image_paths[batch_start:batch_end]
                future = executor.submit(self._extract_batch_features, batch_paths)
                future_to_batch[future] = (batch_paths, batch_start, batch_end)
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            completed = 0
            for future in as_completed(future_to_batch):
                batch_paths, batch_start, batch_end = future_to_batch[future]
                try:
                    batch_features = future.result()
                    
                    # ä¿å­˜åˆ°å­—å…¸
                    for img_path, features in zip(batch_paths, batch_features):
                        if base_dir:
                            rel_path = os.path.relpath(img_path, base_dir)
                        else:
                            rel_path = os.path.basename(img_path)
                        features_dict[rel_path] = features
                    
                    completed += len(batch_paths)
                    if show_progress:
                        print(f"  [{completed}/{total_images}] å¤„ç†å®Œæˆ")
                except Exception as e:
                    print(f"  âš  å¤„ç†æ‰¹æ¬¡å¤±è´¥ {batch_start}-{batch_end}: {e}")
        
        return features_dict
    
    def _extract_batch_features(self, image_paths):
        """
        å†…éƒ¨æ‰¹é‡ç‰¹å¾æå–æ–¹æ³•
        
        å‚æ•°:
            image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
            
        è¿”å›:
            batch_features: æ‰¹é‡ç‰¹å¾å‘é‡åˆ—è¡¨
        """
        batch_features = []
        valid_images = []
        valid_indices = []
        
        # åŠ è½½å’Œé¢„å¤„ç†å›¾ç‰‡
        for i, image_path in enumerate(image_paths):
            if not os.path.exists(image_path):
                print(f"âš  å›¾ç‰‡ä¸å­˜åœ¨: {image_path}")
                batch_features.append(None)  # å ä½ç¬¦
                continue
            
            # åŠ è½½å›¾ç‰‡
            try:
                image = Image.open(image_path).convert('RGB')
                valid_images.append(image)
                valid_indices.append(i)
                batch_features.append(None)  # å ä½ç¬¦
            except Exception as e:
                print(f"âš  æ— æ³•åŠ è½½å›¾ç‰‡ {image_path}: {e}")
                batch_features.append(None)  # å ä½ç¬¦
        
        if not valid_images:
            return batch_features
        
        # é¢„å¤„ç†å›¾ç‰‡
        input_tensors = []
        for image in valid_images:
            input_tensor = self.preprocess(image)
            input_tensors.append(input_tensor)
        
        # è½¬æ¢ä¸ºæ‰¹æ¬¡å¼ é‡
        input_batch = torch.stack(input_tensors).to(self.device)
        
        # æå–ç‰¹å¾ï¼Œä½¿ç”¨çº¿ç¨‹é”ç¡®ä¿çº¿ç¨‹å®‰å…¨
        try:
            with torch.no_grad():
                with self.model_lock:
                    features = self.model(input_batch)
                    
                    # æ ¹æ®æ¨¡å‹ç±»å‹å¤„ç†è¾“å‡º
                    if self.model_name == 'resnet50':
                        # [batch_size, 2048, 1, 1] -> [batch_size, 2048]
                        features = features.squeeze()
                        if len(features.shape) == 1:  # å¤„ç†æ‰¹æ¬¡å¤§å°ä¸º1çš„æƒ…å†µ
                            features = features.unsqueeze(0)
                    elif self.model_name == 'efficientnet_lite0':
                        # EfficientNet ç§»é™¤ _fc å±‚åï¼Œè¾“å‡ºå·²ç»æ˜¯ [batch_size, 1280]
                        if len(features.shape) == 4:
                            features = features.mean([2, 3])  # å…¨å±€å¹³å‡æ± åŒ–
                    elif self.model_name == 'mobilenet_v3_small':
                        # MobileNetV3-Small ç»è¿‡è‡ªå®šä¹‰ç»“æ„åï¼Œè¾“å‡º [batch_size, 576, 1, 1]
                        features = features.squeeze()
                        if len(features.shape) == 1:  # å¤„ç†æ‰¹æ¬¡å¤§å°ä¸º1çš„æƒ…å†µ
                            features = features.unsqueeze(0)
                    elif self.model_name == 'convnext_tiny':
                        # ConvNeXt-Tiny è¾“å‡º [batch_size, 768, 1, 1]ï¼Œéœ€è¦å…¨å±€å¹³å‡æ± åŒ–
                        if len(features.shape) == 4:
                            features = features.mean([2, 3])  # å…¨å±€å¹³å‡æ± åŒ–
                    elif self.model_name in ['openclip_vit_b_32', 'openclip_vit_l_14']:
                        # OpenCLIP æ¨¡å‹è¾“å‡ºç‰¹å¾å‘é‡
                        if isinstance(features, tuple):
                            # å¯¹äºæŸäº›CLIPæ¨¡å‹ï¼Œè¾“å‡ºå¯èƒ½æ˜¯å…ƒç»„
                            features = features[0]  # å–ç¬¬ä¸€ä¸ªå…ƒç´ ä½œä¸ºç‰¹å¾
                        
                        if len(features.shape) == 4:
                            features = features.mean([2, 3])  # å…¨å±€å¹³å‡æ± åŒ–
                        elif len(features.shape) == 3:
                            # å¯¹äºæŸäº›CLIPæ¨¡å‹ï¼Œè¾“å‡ºå¯èƒ½æ˜¯ [batch_size, seq_len, hidden_dim]
                            # å–CLS tokençš„ç‰¹å¾ï¼ˆç¬¬ä¸€ä¸ªtokenï¼‰
                            features = features[:, 0]  # å–ç¬¬ä¸€ä¸ªtoken
        
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            features_np = features.cpu().numpy()
            
            # å¡«å……ç»“æœ
            for i, idx in enumerate(valid_indices):
                feature = features_np[i]
                batch_features[idx] = feature
        except Exception as e:
            print(f"âŒ æ‰¹é‡ç‰¹å¾æå–å¤±è´¥: {e}")
            print(f"  æ‰¹æ¬¡å¤§å°: {len(image_paths)}")
            print(f"  æœ‰æ•ˆå›¾ç‰‡æ•°: {len(valid_images)}")
            print(f"  æ¨¡å‹ç±»å‹: {self.model_name}")
            print(f"  è®¾å¤‡: {self.device}")
            
            # æ‰“å°å‰å‡ å¼ å›¾ç‰‡çš„è·¯å¾„ï¼Œä»¥ä¾¿å®šä½é—®é¢˜å›¾ç‰‡
            if image_paths:
                print(f"  å‰5å¼ å›¾ç‰‡è·¯å¾„:")
                for path in image_paths[:5]:
                    print(f"    - {path}")
            
            # ä¸ºå¤±è´¥çš„æ‰¹æ¬¡è¿”å›None
            for i in range(len(batch_features)):
                batch_features[i] = None
        
        return batch_features
    
    def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'name': self.model_name,
            'device': str(self.device),
            'feature_dim': self.feature_dim,
            'cached': self.cache_manager.is_model_cached(self.model_name)
        }
    
    # def generate_contrastive_pairs(self, image_paths):
    #     """
    #     ç”Ÿæˆå¯¹æ¯”å­¦ä¹ çš„æ­£æ ·æœ¬å¯¹
        
    #     å‚æ•°:
    #         image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
            
    #     è¿”å›:
    #         pairs: å…ƒç»„åˆ—è¡¨ [(aug1, aug2) ...]ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯åŒä¸€å›¾ç‰‡çš„ä¸¤ä¸ªå¢å¼ºç‰ˆæœ¬
    #     """
    #     self._initialize_contrastive_augmentation()
    #     pairs = []
        
    #     for img_path in image_paths:
    #         try:
    #             # åŠ è½½åŸå§‹å›¾ç‰‡
    #             img = Image.open(img_path).convert('RGB')
                
    #             # ç”Ÿæˆä¸¤ä¸ªå¢å¼ºç‰ˆæœ¬
    #             aug1 = self.contrastive_augmentation(img)
    #             aug2 = self.contrastive_augmentation(img)
                
    #             pairs.append((aug1, aug2))
    #         except Exception as e:
    #             print(f"âš  ç”Ÿæˆå¯¹æ¯”æ ·æœ¬å¯¹å¤±è´¥ {img_path}: {e}")
        
    #     return pairs
    
    # def train_contrastive(self, image_paths, epochs=10, learning_rate=1e-4, batch_size=32, temperature=0.5):
    #     """
    #     ä½¿ç”¨å¯¹æ¯”å­¦ä¹ è®­ç»ƒæ¨¡å‹
        
    #     å‚æ•°:
    #         image_paths: è®­ç»ƒå›¾ç‰‡è·¯å¾„åˆ—è¡¨
    #         epochs: è®­ç»ƒè½®æ•°
    #         learning_rate: å­¦ä¹ ç‡
    #         batch_size: æ‰¹æ¬¡å¤§å°
    #         temperature: å¯¹æ¯”æŸå¤±çš„æ¸©åº¦å‚æ•°
            
    #     è¿”å›:
    #         history: è®­ç»ƒå†å²ï¼ŒåŒ…å«æ¯è½®çš„æŸå¤±å€¼
    #     """
    #     import torch.optim as optim
    #     import torch.utils.data as data
        
    #     print(f"\n{'='*60}")
    #     print("ğŸ”§ å¼€å§‹å¯¹æ¯”å­¦ä¹ è®­ç»ƒ")
    #     print(f"{'='*60}")
        
    #     # å°†æ¨¡å‹åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼
    #     self.model.train()
        
    #     # ç”Ÿæˆå¯¹æ¯”æ ·æœ¬å¯¹
    #     pairs = self.generate_contrastive_pairs(image_paths)
        
    #     if not pairs:
    #         print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•å¯¹æ¯”æ ·æœ¬å¯¹ï¼Œè®­ç»ƒå¤±è´¥")
    #         return None
        
    #     # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    #     dataset = data.TensorDataset(
    #         torch.stack([pair[0] for pair in pairs]),
    #         torch.stack([pair[1] for pair in pairs])
    #     )
    #     dataloader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
    #     # åˆå§‹åŒ–ä¼˜åŒ–å™¨
    #     optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        
    #     # åˆå§‹åŒ–ContrastiveLearner
    #     from torch.nn.functional import normalize
        
    #     history = []
    #     pytor
        
    #     for epoch in range(epochs):
    #         running_loss = 0.0
            
    #         for i, (aug1, aug2) in enumerate(dataloader):
    #             # å°†æ•°æ®ç§»åˆ°è®¾å¤‡
    #             aug1 = aug1.to(self.device)
    #             aug2 = aug2.to(self.device)
                
    #             # é›¶æ¢¯åº¦
    #             optimizer.zero_grad()
                
    #             # æå–ç‰¹å¾
    #             features1 = self.model(aug1)
    #             features2 = self.model(aug2)
                
    #             # å¤„ç†ç‰¹å¾æ ¼å¼
    #             if self.model_name.startswith('vgg'):
    #                 features1 = features1.view(features1.size(0), -1)
    #                 features2 = features2.view(features2.size(0), -1)
    #             elif self.model_name.startswith('resnet'):
    #                 features1 = features1.squeeze()
    #                 features2 = features2.squeeze()
    #             elif self.model_name == 'mobilenet_v2':
    #                 features1 = features1.mean([2, 3])
    #                 features2 = features2.mean([2, 3])
                
    #             # æ‹¼æ¥ç‰¹å¾
    #             features = torch.cat([features1, features2], dim=0)
                
    #             # L2å½’ä¸€åŒ–
    #             features = normalize(features, dim=1)
                
    #             # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    #             similarity_matrix = torch.mm(features, features.t()) / temperature
                
    #             # åˆ›å»ºæ ‡ç­¾
    #             batch_size = features1.size(0)
    #             labels = torch.cat([torch.arange(batch_size), torch.arange(batch_size)], dim=0)
                
    #             # åˆ›å»ºæ©ç çŸ©é˜µ
    #             mask = labels.unsqueeze(0) == labels.unsqueeze(1)
    #             mask = mask.fill_diagonal_(0)
                
    #             # è®¡ç®—æ­£æ ·æœ¬
    #             positive_pairs = similarity_matrix[mask]
                
    #             # è®¡ç®—è´Ÿæ ·æœ¬
    #             exp_similarity = torch.exp(similarity_matrix)
    #             sum_exp = torch.sum(exp_similarity, dim=1) - torch.exp(similarity_matrix.diag())
                
    #             # è·å–æ­£æ ·æœ¬å¯¹çš„ç´¢å¼•
    #             positive_indices = torch.nonzero(mask, as_tuple=True)
    #             sum_exp_positive = sum_exp[positive_indices[0]]
                
    #             # è®¡ç®—æŸå¤±
    #             loss = -torch.log(positive_pairs / sum_exp_positive)
    #             loss = loss.mean()
                
    #             # åå‘ä¼ æ’­
    #             loss.backward()
    #             optimizer.step()
                
    #             running_loss += loss.item()
                
    #         # è®¡ç®—å¹³å‡æŸå¤±
    #         avg_loss = running_loss / len(dataloader)
    #         history.append(avg_loss)
            
    #         print(f"Epoch {epoch+1:3d}/{epochs:3d} | Loss: {avg_loss:.6f}")
        
    #     # å°†æ¨¡å‹åˆ‡å›è¯„ä¼°æ¨¡å¼
    #     self.model.eval()
        
    #     print(f"\nâœ… å¯¹æ¯”å­¦ä¹ è®­ç»ƒå®Œæˆ")
    #     print(f"æœ€åçš„æŸå¤±å€¼: {history[-1]:.6f}")
        
    #     return history
    
    def save_trained_model(self, save_path):
        """
        ä¿å­˜è®­ç»ƒåçš„æ¨¡å‹
        
        å‚æ•°:
            save_path: ä¿å­˜è·¯å¾„
        """
        torch.save(self.model.state_dict(), save_path)
        print(f"âœ“ è®­ç»ƒåçš„æ¨¡å‹å·²ä¿å­˜: {save_path}")
    
    def load_trained_model(self, load_path):
        """
        åŠ è½½è®­ç»ƒåçš„æ¨¡å‹
        
        å‚æ•°:
            load_path: åŠ è½½è·¯å¾„
        """
        self.model.load_state_dict(torch.load(load_path, map_location=self.device))
        print(f"âœ“ è®­ç»ƒåçš„æ¨¡å‹å·²åŠ è½½: {load_path}")

# ========== 4. ç›¸ä¼¼åº¦åˆ†æå™¨ (ä¿æŒä¸å˜) ==========
class SimilarityAnalyzer:
    """ç›¸ä¼¼åº¦åˆ†æå™¨"""
    
    def __init__(self):
        self.similarity_methods = {
            'cosine': self.cosine_similarity,
            'euclidean': self.euclidean_distance,
            'manhattan': self.manhattan_distance,
        }
    
    def cosine_similarity(self, vec1, vec2):
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        return dot_product / (norm1 * norm2)
    
    def euclidean_distance(self, vec1, vec2):
        return np.linalg.norm(vec1 - vec2)
    
    def manhattan_distance(self, vec1, vec2):
        return np.sum(np.abs(vec1 - vec2))
    
    def compute_similarity_matrix(self, features_dict, method='cosine'):
        if method not in self.similarity_methods:
            raise ValueError(f"ä¸æ”¯æŒçš„ç›¸ä¼¼åº¦æ–¹æ³•: {method}")
        
        image_names = list(features_dict.keys())
        n_images = len(image_names)
        similarity_func = self.similarity_methods[method]
        
        similarity_matrix = np.zeros((n_images, n_images))
        
        for i in range(n_images):
            for j in range(n_images):
                if i == j:
                    if method == 'cosine':
                        similarity_matrix[i, j] = 1.0
                    else:
                        similarity_matrix[i, j] = 0.0
                else:
                    sim = similarity_func(
                        features_dict[image_names[i]], 
                        features_dict[image_names[j]]
                    )
                    similarity_matrix[i, j] = sim
        
        return similarity_matrix, image_names
    
    def print_similarity_analysis(self, features_dict, method='cosine'):
        print(f"\n{'='*60}")
        print(f"ç›¸ä¼¼åº¦åˆ†æ (æ–¹æ³•: {method})")
        print(f"{'='*60}")
        
        image_names = list(features_dict.keys())
        similarity_func = self.similarity_methods[method]
        
        dog_images = [name for name in image_names if 'dog' in name.lower()]
        cat_images = [name for name in image_names if 'cat' in name.lower()]
        
        print(f"\nå›¾ç‰‡åˆ†ç±»:")
        print(f"  ç‹—ç±»å›¾ç‰‡ ({len(dog_images)}å¼ ): {', '.join(dog_images)}")
        print(f"  çŒ«ç±»å›¾ç‰‡ ({len(cat_images)}å¼ ): {', '.join(cat_images)}")
        
        print(f"\nç±»å†…ç›¸ä¼¼åº¦ (åŒç±»å›¾ç‰‡ä¹‹é—´çš„ç›¸ä¼¼åº¦):")
        
        if len(dog_images) > 1:
            dog_similarities = []
            for i in range(len(dog_images)):
                for j in range(i+1, len(dog_images)):
                    sim = similarity_func(
                        features_dict[dog_images[i]], 
                        features_dict[dog_images[j]]
                    )
                    dog_similarities.append(sim)
            avg_dog_sim = np.mean(dog_similarities)
            print(f"  ç‹—ç±»å¹³å‡ç›¸ä¼¼åº¦: {avg_dog_sim:.4f} (èŒƒå›´: {np.min(dog_similarities):.4f} - {np.max(dog_similarities):.4f})")
        
        if len(cat_images) > 1:
            cat_similarities = []
            for i in range(len(cat_images)):
                for j in range(i+1, len(cat_images)):
                    sim = similarity_func(
                        features_dict[cat_images[i]], 
                        features_dict[cat_images[j]]
                    )
                    cat_similarities.append(sim)
            avg_cat_sim = np.mean(cat_similarities)
            print(f"  çŒ«ç±»å¹³å‡ç›¸ä¼¼åº¦: {avg_cat_sim:.4f} (èŒƒå›´: {np.min(cat_similarities):.4f} - {np.max(cat_similarities):.4f})")
        
        print(f"\nç±»é—´ç›¸ä¼¼åº¦ (ä¸åŒç±»å›¾ç‰‡ä¹‹é—´çš„ç›¸ä¼¼åº¦):")
        cross_similarities = []
        for dog_img in dog_images:
            for cat_img in cat_images:
                sim = similarity_func(
                    features_dict[dog_img], 
                    features_dict[cat_img]
                )
                cross_similarities.append(sim)
        
        if cross_similarities:
            avg_cross_sim = np.mean(cross_similarities)
            print(f"  ç‹—çŒ«å¹³å‡ç›¸ä¼¼åº¦: {avg_cross_sim:.4f} (èŒƒå›´: {np.min(cross_similarities):.4f} - {np.max(cross_similarities):.4f})")
            
            if len(dog_images) > 1 and len(cat_images) > 1:
                avg_within = (avg_dog_sim + avg_cat_sim) / 2
                # contrast_score = avg_within - avg_cross_sim  # å¯¹æ¯”å­¦ä¹ ç›¸å…³ï¼Œå·²æ³¨é‡Š
                # å¯¹æ¯”å­¦ä¹ æŒ‡æ ‡éƒ¨åˆ†å·²æ³¨é‡Š
                # print(f"\nå¯¹æ¯”å­¦ä¹ æŒ‡æ ‡:")
                # print(f"  ç±»å†…å¹³å‡ç›¸ä¼¼åº¦: {avg_within:.4f}")
                # print(f"  ç±»é—´å¹³å‡ç›¸ä¼¼åº¦: {avg_cross_sim:.4f}")
                # print(f"  å¯¹æ¯”å¾—åˆ† (ç±»å†…-ç±»é—´): {contrast_score:.4f}")
                # if contrast_score > 0:
                #     print(f"  âœ“ æ¨¡å‹èƒ½å¤ŸåŒºåˆ†çŒ«ç‹— (å¯¹æ¯”å¾—åˆ†ä¸ºæ­£)")
                # else:
                #     print(f"  âš  æ¨¡å‹éš¾ä»¥åŒºåˆ†çŒ«ç‹— (å¯¹æ¯”å¾—åˆ†ä¸ºè´Ÿæˆ–é›¶)")

# ========== 5.1 å¯¹æ¯”å­¦ä¹ å®ç° (å·²æ³¨é‡Š) ==========
# class ContrastiveLearner:
#     """ç®€å•çš„å¯¹æ¯”å­¦ä¹ å®ç°"""
    
#     def __init__(self):
#         self.temperature = 0.5  # æ¸©åº¦å‚æ•°ï¼Œç”¨äºæ§åˆ¶ç›¸ä¼¼åº¦åˆ†å¸ƒçš„å¹³æ»‘ç¨‹åº¦
    
#     def contrastive_loss(self, features, labels):
#         """
#         è®¡ç®—å¯¹æ¯”æŸå¤±
        
#         å‚æ•°:
#             features: ç‰¹å¾å‘é‡åˆ—è¡¨ï¼Œå½¢çŠ¶ä¸º [batch_size, feature_dim]
#             labels: æ ·æœ¬æ ‡ç­¾åˆ—è¡¨ï¼Œå½¢çŠ¶ä¸º [batch_size]
            
#         è¿”å›:
#             loss: å¯¹æ¯”æŸå¤±å€¼
#         """
#         import torch
        
#         # å°†ç‰¹å¾è½¬æ¢ä¸ºå¼ é‡
#         features = torch.tensor(features)
#         labels = torch.tensor(labels)
        
#         # è®¡ç®—ç‰¹å¾å‘é‡çš„L2èŒƒæ•°-å½’ä¸€åŒ–
#         features = features / torch.norm(features, dim=1, keepdim=True)
        
#         # è®¡ç®—ç›¸ä¼¼æ€§çŸ©é˜µ
#         similarity_matrix = torch.mm(features, features.t()) / self.temperature
        
#         # åˆ›å»ºæ©ç çŸ©é˜µï¼ŒåŒºåˆ†æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬
#         mask = labels.unsqueeze(0) == labels.unsqueeze(1)
#         mask = mask.fill_diagonal_(0)  # æ’é™¤è‡ªèº«
        
#         # è®¡ç®—æ­£æ ·æœ¬æŸå¤±
#         positive_pairs = similarity_matrix[mask]
#         if len(positive_pairs) == 0:
#             return 0.0  # æ²¡æœ‰æ­£æ ·æœ¬å¯¹æ—¶ï¼ŒæŸå¤±ä¸º0
        
#         # è®¡ç®—è´Ÿæ ·æœ¬æŸå¤±
#         exp_similarity = torch.exp(similarity_matrix)
#         sum_exp = torch.sum(exp_similarity, dim=1) - torch.exp(similarity_matrix.diag())
        
#         # è·å–æ­£æ ·æœ¬å¯¹çš„ç´¢å¼•
#         positive_indices = torch.nonzero(mask, as_tuple=True)
        
#         # ä¸ºæ¯ä¸ªæ­£æ ·æœ¬å¯¹è·å–å¯¹åº”çš„sum_expå€¼
#         sum_exp_positive = sum_exp[positive_indices[0]]
        
#         # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŸå¤±
#         loss = -torch.log(positive_pairs / sum_exp_positive)
        
#         return torch.mean(loss).item()
    
#     def create_contrastive_pairs(self, features_dict):
#         """
#         åˆ›å»ºå¯¹æ¯”å­¦ä¹ çš„æ•°æ®å¯¹
        
#         å‚æ•°:
#             features_dict: ç‰¹å¾å­—å…¸ {å›¾ç‰‡å: ç‰¹å¾å‘é‡}
            
#         è¿”å›:
#             features: ç‰¹å¾å‘é‡æ•°ç»„
#             labels: æ ·æœ¬æ ‡ç­¾æ•°ç»„ (0=cat, 1=dog)
#         """
#         features = []
#         labels = []
        
#         for img_name, feature in features_dict.items():
#             features.append(feature)
#             # æ ¹æ®æ–‡ä»¶ååˆ¤æ–­æ ‡ç­¾
#             if 'cat' in img_name.lower():
#                 labels.append(0)
#             elif 'dog' in img_name.lower():
#                 labels.append(1)
#             else:
#                 labels.append(2)  # å…¶ä»–ç±»åˆ«
        
#         return np.array(features), np.array(labels)
    
#     def run_contrastive_learning_example(self, features_dict):
#         """
#         è¿è¡Œå¯¹æ¯”å­¦ä¹ ç¤ºä¾‹
        
#         å‚æ•°:
#             features_dict: ç‰¹å¾å­—å…¸ {å›¾ç‰‡å: ç‰¹å¾å‘é‡}
#         """
#         print(f"\n{'='*60}")
#         print("ğŸ”¬ å¯¹æ¯”å­¦ä¹ ç¤ºä¾‹")
#         print(f"{'='*60}")
        
#         # åˆ›å»ºå¯¹æ¯”å­¦ä¹ æ•°æ®
#         features, labels = self.create_contrastive_pairs(features_dict)
        
#         print(f"å¯¹æ¯”å­¦ä¹ æ•°æ®:")
#         print(f"  - æ€»æ ·æœ¬æ•°: {len(features)}")
#         print(f"  - ç‰¹å¾ç»´åº¦: {features.shape[1]}")
#         print(f"  - çŒ«ç±»æ ·æœ¬ (æ ‡ç­¾0): {np.sum(labels == 0)}ä¸ª")
#         print(f"  - ç‹—ç±»æ ·æœ¬ (æ ‡ç­¾1): {np.sum(labels == 1)}ä¸ª")
        
#         # è®¡ç®—å¯¹æ¯”æŸå¤±
#         if len(features) < 2:
#             print("âš  æ ·æœ¬æ•°ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå¯¹æ¯”å­¦ä¹ ")
#             return
        
#         loss = self.contrastive_loss(features, labels)
#         print(f"\nå¯¹æ¯”æŸå¤±å€¼: {loss:.4f}")
        
#         # è§£é‡Šç»“æœ
#         print(f"\nå¯¹æ¯”å­¦ä¹ ç»“æœåˆ†æ:")
#         if loss < 1.0:
#             print("  âœ“ ç‰¹å¾å‘é‡å…·æœ‰è¾ƒå¥½çš„åŒºåˆ†èƒ½åŠ›")
#         elif loss < 2.0:
#             print("  âš  ç‰¹å¾å‘é‡çš„åŒºåˆ†èƒ½åŠ›ä¸€èˆ¬")
#         else:
#             print("  âŒ ç‰¹å¾å‘é‡çš„åŒºåˆ†èƒ½åŠ›è¾ƒå¼±")
        
#         print(f"\nğŸ“ å¯¹æ¯”å­¦ä¹ è¯´æ˜:")
#         print(f"  - å¯¹æ¯”å­¦ä¹ ç›®æ ‡æ˜¯è®©åŒç±»æ ·æœ¬çš„ç‰¹å¾æ›´ç›¸ä¼¼ï¼Œä¸åŒç±»æ ·æœ¬çš„ç‰¹å¾æ›´ä¸åŒ")
#         print(f"  - æŸå¤±å€¼è¶Šå°ï¼Œè¡¨ç¤ºç‰¹å¾çš„åŒºåˆ†èƒ½åŠ›è¶Šå¼º")
#         print(f"  - æ¸©åº¦å‚æ•°({self.temperature})æ§åˆ¶ç›¸ä¼¼åº¦åˆ†å¸ƒçš„å¹³æ»‘ç¨‹åº¦")

# ========== 5.2 å¯è§†åŒ–å·¥å…· (ä¼˜åŒ–) ==========
class Visualizer:
    """å¯è§†åŒ–å·¥å…·"""
    
    def __init__(self):
        self.colors = {
            'dog': '#FF6B6B',
            'cat': '#4ECDC4',
        }
    
    def create_similarity_heatmap(self, similarity_matrix, image_names, model_name, save_dir='./output'):
        """åˆ›å»ºç›¸ä¼¼åº¦çƒ­å›¾"""
        # os.makedirs(save_dir, exist_ok=True)
        # save_path = os.path.join(save_dir, f'similarity_heatmap_{model_name}.png')
        
        # plt.figure(figsize=(10, 8))
        # im = plt.imshow(similarity_matrix, cmap='YlOrRd', vmin=0, vmax=1)
        
        # short_names = [n.split('.')[0] for n in image_names]
        # plt.xticks(range(len(image_names)), short_names, rotation=45, ha='right')
        # plt.yticks(range(len(image_names)), short_names)
        
        # for i in range(len(image_names)):
        #     for j in range(len(image_names)):
        #         color = 'black' if similarity_matrix[i, j] < 0.7 else 'white'
        #         plt.text(j, i, f'{similarity_matrix[i, j]:.2f}', 
        #                 ha='center', va='center', color=color, fontsize=9)
        
        # plt.colorbar(im, fraction=0.046, pad=0.04)
        # plt.title(f'å›¾ç‰‡ç›¸ä¼¼åº¦çŸ©é˜µ - {model_name}', fontsize=14, fontweight='bold')
        # plt.tight_layout()
        # plt.savefig(save_path, dpi=300, bbox_inches='tight')
        # plt.close()
        # print(f"âœ“ ç›¸ä¼¼åº¦çƒ­å›¾å·²ä¿å­˜: {save_path}")
        print(f"âš  ç›¸ä¼¼åº¦çƒ­å›¾åŠŸèƒ½å·²ç¦ç”¨")
    
    def create_feature_scatter(self, features_dict, model_name, save_dir='./output'):
        """åˆ›å»ºç‰¹å¾æ•£ç‚¹åˆ†å¸ƒå›¾"""
        # os.makedirs(save_dir, exist_ok=True)
        # save_path = os.path.join(save_dir, f'feature_scatter_{model_name}.png')
        
        # image_names = list(features_dict.keys())
        # features = np.array([features_dict[name] for name in image_names])
        
        # pca = PCA(n_components=2)
        # features_2d = pca.fit_transform(features)
        
        # plt.figure(figsize=(12, 10))
        
        # for i, img_name in enumerate(image_names):
        #     if 'dog' in img_name.lower():
        #         color = self.colors['dog']
        #         marker = 'o'
        #     else:
        #         color = self.colors['cat']
        #         marker = 's'
            
        #     plt.scatter(features_2d[i, 0], features_2d[i, 1], 
        #                color=color, s=200, marker=marker, 
        #                edgecolor='black', linewidth=1.5, alpha=0.8)
            
        #     plt.annotate(img_name.split('.')[0], 
        #                 xy=(features_2d[i, 0], features_2d[i, 1]),
        #                 xytext=(5, 5), textcoords='offset points',
        #                 fontsize=10, bbox=dict(boxstyle="round,pad=0.3", 
        #                                       facecolor='white', 
        #                                       edgecolor='gray', alpha=0.8))
        
        # from matplotlib.patches import Patch 
        # legend_elements = [
        #     Patch(facecolor=self.colors['dog'], edgecolor='black', label='ç‹—'),
        #     Patch(facecolor=self.colors['cat'], edgecolor='black', label='çŒ«'),
        # ]
        # plt.legend(handles=legend_elements, loc='upper right')
        
        # explained_var = pca.explained_variance_ratio_
        # plt.title(f'ç‰¹å¾ç©ºé—´åˆ†å¸ƒ - {model_name}', fontsize=16, fontweight='bold')
        # plt.xlabel(f'ä¸»æˆåˆ† 1 ({explained_var[0]:.2%})', fontsize=12)
        # plt.ylabel(f'ä¸»æˆåˆ† 2 ({explained_var[1]:.2%})', fontsize=12)
        # plt.grid(True, alpha=0.3, linestyle='--')
        
        # plt.tight_layout()
        # plt.savefig(save_path, dpi=300, bbox_inches='tight')
        # plt.close()
        # print(f"âœ“ ç‰¹å¾æ•£ç‚¹å›¾å·²ä¿å­˜: {save_path}")
        print(f"âš  ç‰¹å¾æ•£ç‚¹å›¾åŠŸèƒ½å·²ç¦ç”¨")
    
    def create_tsne_visualization(self, features_dict, model_name, save_dir='./output'):
        """åˆ›å»ºt-SNEå¯è§†åŒ–"""
        # if len(features_dict) < 3:
        #     print("âš  æ ·æœ¬æ•°ä¸è¶³ï¼Œè·³è¿‡t-SNEå¯è§†åŒ–")
        #     return
        
        # os.makedirs(save_dir, exist_ok=True)
        # save_path = os.path.join(save_dir, f'tsne_visualization_{model_name}.png')
        
        # image_names = list(features_dict.keys())
        # features = np.array([features_dict[name] for name in image_names])
        
        # tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(features)-1))
        # features_2d = tsne.fit_transform(features)
        
        # plt.figure(figsize=(12, 10))
        
        # for i, img_name in enumerate(image_names):
        #     if 'dog' in img_name.lower():
        #         color = self.colors['dog']
        #         marker = 'o'
        #     else:
        #         color = self.colors['cat']
        #         marker = 's'
            
        #     plt.scatter(features_2d[i, 0], features_2d[i, 1], 
        #                color=color, s=200, marker=marker,
        #                edgecolor='black', linewidth=1.5, alpha=0.8)
            
        #     plt.annotate(img_name.split('.')[0], 
        #                 xy=(features_2d[i, 0], features_2d[i, 1]),
        #                 xytext=(5, 5), textcoords='offset points',
        #                 fontsize=10, bbox=dict(boxstyle="round,pad=0.3", 
        #                                       facecolor='white', 
        #                                       edgecolor='gray', alpha=0.8))
        
        # plt.title(f'ç‰¹å¾ç©ºé—´åˆ†å¸ƒ (t-SNE) - {model_name}', fontsize=16, fontweight='bold')
        # plt.xlabel('t-SNE 1', fontsize=12)
        # plt.ylabel('t-SNE 2', fontsize=12)
        # plt.grid(True, alpha=0.3, linestyle='--')
        
        # plt.tight_layout()
        # plt.savefig(save_path, dpi=300, bbox_inches='tight')
        # plt.close()
        # print(f"âœ“ t-SNEå¯è§†åŒ–å·²ä¿å­˜: {save_path}")
        print(f"âš  t-SNEå¯è§†åŒ–åŠŸèƒ½å·²ç¦ç”¨")

# ========== 6. ä¸»ç¨‹åº (ä¼˜åŒ–) ==========
def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ±ğŸ¶ çŒ«ç‹—å›¾ç‰‡ç‰¹å¾æå–ä¸ç›¸ä¼¼åº¦åˆ†æ")
    print("=" * 60)
    
    # åˆ›å»ºç¼“å­˜ç®¡ç†å™¨
    cache_manager = ModelCacheManager()
    
    # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹
    print(f"\nğŸ“Š æ”¯æŒçš„æ¨¡å‹:")
    supported_models = FeatureExtractor.SUPPORTED_MODELS
    for i, model in enumerate(supported_models, 1):
        cached = "âœ“" if cache_manager.is_model_cached(model) else " "
        print(f"  {cached} {i:2d}. {model:12}", end="")
        if i % 3 == 0:
            print()
    
    # é€‰æ‹©æ¨¡å‹
    print(f"\n\nğŸ”§ è¯·é€‰æ‹©æ¨¡å‹ (1-{len(supported_models)}):")
    for i, model in enumerate(supported_models, 1):
        print(f"  {i:2d}. {model}")
    
    try:
        choice = int(input(f"\nè¯·è¾“å…¥é€‰æ‹© (é»˜è®¤1): ") or "1")
        if 1 <= choice <= len(supported_models):
            model_name = supported_models[choice-1]
        else:
            print(f"âš  è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤: resnet18")
            model_name = "resnet18"
    except:
        print(f"âš  è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤: resnet18")
        model_name = "resnet18"
    
    # é€‰æ‹©è®¾å¤‡
    device_options = []
    if torch.backends.mps.is_available():
        device_options.append(("1", "mps", "Apple Silicon"))
    if torch.cuda.is_available():
        device_options.append(("2", "cuda", "NVIDIA GPU"))
    device_options.append(("3", "cpu", "CPU"))
    
    print(f"\nğŸ’» è¯·é€‰æ‹©è®¾å¤‡:")
    for idx, device_code, description in device_options:
        print(f"  {idx}. {description} ({device_code})")
    
    device_choice = input(f"\nè¯·è¾“å…¥é€‰æ‹© (é»˜è®¤1): ") or "1"
    device_map = {idx: device_code for idx, device_code, _ in device_options}
    device = device_map.get(device_choice, "auto")
    
    # æ£€æŸ¥å›¾ç‰‡ç›®å½•
    img_dir = "img"
    if not os.path.exists(img_dir):
        print(f"\nâŒ é”™è¯¯: å›¾ç‰‡ç›®å½• '{img_dir}' ä¸å­˜åœ¨")
        print("è¯·åˆ›å»º img/ ç›®å½•å¹¶æ”¾å…¥å›¾ç‰‡")
        return
    
    # æŸ¥æ‰¾å›¾ç‰‡
    import glob
    image_patterns = ["*.png", "*.jpg", "*.jpeg", "*.bmp", "*.tiff"]
    image_paths = []
    for pattern in image_patterns:
        image_paths.extend(glob.glob(os.path.join(img_dir, pattern)))
    
    if not image_paths:
        print(f"\nâŒ åœ¨ {img_dir} ä¸­æœªæ‰¾åˆ°å›¾ç‰‡")
        return
    
    # æŒ‰åç§°æ’åº
    image_paths.sort()
    print(f"\nğŸ“¸ æ‰¾åˆ° {len(image_paths)} å¼ å›¾ç‰‡:")
    for i, path in enumerate(image_paths[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
        print(f"  {i:2d}. {os.path.basename(path)}")
    if len(image_paths) > 10:
        print(f"  ... å’Œ {len(image_paths)-10} å¼ æ›´å¤šå›¾ç‰‡")
    
    # ç¡®è®¤æ˜¯å¦ç»§ç»­
    confirm = input(f"\næ˜¯å¦ç»§ç»­å¤„ç†è¿™ {len(image_paths)} å¼ å›¾ç‰‡? (y/n, é»˜è®¤y): ") or "y"
    if confirm.lower() != 'y':
        print("ğŸ‘‹ å·²å–æ¶ˆ")
        return
    
    # åˆ›å»ºç‰¹å¾æå–å™¨
    print(f"\n{'='*30}")
    print(f"åˆå§‹åŒ– {model_name} ç‰¹å¾æå–å™¨")
    print(f"{'='*30}")
    
    try:
        extractor = FeatureExtractor(
            model_name=model_name,
            device=device,
            cache_dir="./model_cache"
        )
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # æå–ç‰¹å¾
    print(f"\n{'='*30}")
    print("æå–ç‰¹å¾")
    print(f"{'='*30}")
    
    features_dict = extractor.extract_batch_features(image_paths, show_progress=True)
    
    if not features_dict:
        print("âŒ æœªèƒ½æå–ä»»ä½•ç‰¹å¾")
        return
    
    print(f"\nâœ… æˆåŠŸæå– {len(features_dict)} å¼ å›¾ç‰‡çš„ç‰¹å¾")
    
    # æ˜¾ç¤ºç‰¹å¾ä¿¡æ¯
    first_key = list(features_dict.keys())[0]
    print(f"ç‰¹å¾ç»´åº¦: {features_dict[first_key].shape}")
    
    # è¯¢é—®æ˜¯å¦è¿›è¡Œå¯¹æ¯”å­¦ä¹ è®­ç»ƒ (å·²æ³¨é‡Š)
    # print(f"\n{'='*60}")
    # print("ğŸ¤” æ˜¯å¦è¿›è¡Œå¯¹æ¯”å­¦ä¹ è®­ç»ƒ?")
    # print(f"{'='*60}")
    
    # train_choice = input("è¾“å…¥ 'y' è¿›è¡Œè®­ç»ƒï¼Œå…¶ä»–ä»»æ„é”®è·³è¿‡ (é»˜è®¤: n): ") or "n"
    
    # if train_choice.lower() == 'y':
    #     # è®­ç»ƒé…ç½®é€‰é¡¹
    #     try:
    #         epochs = int(input("è¾“å…¥è®­ç»ƒè½®æ•° (é»˜è®¤: 10): ") or "10")
    #         learning_rate = float(input("è¾“å…¥å­¦ä¹ ç‡ (é»˜è®¤: 1e-4): ") or "1e-4")
    #         batch_size = int(input("è¾“å…¥æ‰¹æ¬¡å¤§å° (é»˜è®¤: 32): ") or "32")
    #         temperature = float(input("è¾“å…¥æ¸©åº¦å‚æ•° (é»˜è®¤: 0.5): ") or "0.5")
    #     except ValueError:
    #         print("âš  è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
    #         epochs = 10
    #         learning_rate = 1e-4
    #         batch_size = 32
    #         temperature = 0.5
        
    #     # è¿›è¡Œå¯¹æ¯”å­¦ä¹ è®­ç»ƒ
    #     history = extractor.train_contrastive(
    #         image_paths,
    #         epochs=epochs,
    #         learning_rate=learning_rate,
    #         batch_size=batch_size,
    #         temperature=temperature
    #     )
        
    #     # è¯¢é—®æ˜¯å¦ä¿å­˜è®­ç»ƒåçš„æ¨¡å‹
    #     if history:
    #         save_choice = input("\næ˜¯å¦ä¿å­˜è®­ç»ƒåçš„æ¨¡å‹? (y/n, é»˜è®¤: y): ") or "y"
    #         if save_choice.lower() == 'y':
    #             save_path = f"./output/trained_{model_name}.pt"
    #             extractor.save_trained_model(save_path)
        
    #     # é‡æ–°æå–ç‰¹å¾ï¼Œä½¿ç”¨è®­ç»ƒåçš„æ¨¡å‹
    #     print(f"\n{'='*60}")
    #     print("ğŸ”„ ä½¿ç”¨è®­ç»ƒåçš„æ¨¡å‹é‡æ–°æå–ç‰¹å¾")
    #     print(f"{'='*60}")
    #     features_dict = extractor.extract_batch_features(image_paths, show_progress=True)
    
    # ä¿å­˜ç‰¹å¾
    output_dir = "./output"
    os.makedirs(output_dir, exist_ok=True)
    
    features_file = os.path.join(output_dir, f'features_{model_name}.npz')
    np.savez(features_file, **features_dict)
    print(f"âœ“ ç‰¹å¾å·²ä¿å­˜: {features_file}")
    
    # ç›¸ä¼¼åº¦åˆ†æ
    analyzer = SimilarityAnalyzer()
    
    print(f"\n{'='*30}")
    print("ç›¸ä¼¼åº¦åˆ†æ")
    print(f"{'='*30}")
    
    # ä½¿ç”¨ä¸åŒæ–¹æ³•åˆ†æ
    methods = ['cosine', 'euclidean', 'manhattan']
    
    for method in methods:
        try:
            analyzer.print_similarity_analysis(features_dict, method=method)
            
            # è®¡ç®—å¹¶ä¿å­˜ç›¸ä¼¼åº¦çŸ©é˜µ
            similarity_matrix, image_names = analyzer.compute_similarity_matrix(
                features_dict, method=method
            )
            
            matrix_file = os.path.join(output_dir, f'similarity_matrix_{method}_{model_name}.npy')
            np.save(matrix_file, similarity_matrix)
            print(f"âœ“ ç›¸ä¼¼åº¦çŸ©é˜µå·²ä¿å­˜: {matrix_file}")
            
        except Exception as e:
            print(f"è­¦å‘Š: {method} æ–¹æ³•åˆ†æå¤±è´¥: {e}")
    
    # å¯è§†åŒ–
    print(f"\n{'='*30}")
    print("ç”Ÿæˆå¯è§†åŒ–")
    print(f"{'='*30}")
    
    visualizer = Visualizer()
    
    # è·å–ç›¸ä¼¼åº¦çŸ©é˜µç”¨äºå¯è§†åŒ–
    similarity_matrix, image_names = analyzer.compute_similarity_matrix(
        features_dict, method='cosine'
    )
    
    # 1. ç›¸ä¼¼åº¦çƒ­å›¾
    visualizer.create_similarity_heatmap(
        similarity_matrix, image_names, model_name, save_dir=output_dir
    )
    
    # 2. PCAæ•£ç‚¹å›¾
    visualizer.create_feature_scatter(features_dict, model_name, save_dir=output_dir)
    
    # 3. t-SNEå¯è§†åŒ–
    visualizer.create_tsne_visualization(features_dict, model_name, save_dir=output_dir)
    
    # 4. å¯¹æ¯”å­¦ä¹ ç¤ºä¾‹ (å·²æ³¨é‡Š)
    # print(f"\n{'='*30}")
    # print("å¯¹æ¯”å­¦ä¹ ç¤ºä¾‹")
    # print(f"{'='*30}")
    
    # contrastive_learner = ContrastiveLearner()
    # contrastive_learner.run_contrastive_learning_example(features_dict)
    
    # æ‰“å°è¯¦ç»†çš„ç›¸ä¼¼åº¦è¡¨æ ¼
    print(f"\n{'='*60}")
    print("è¯¦ç»†ç›¸ä¼¼åº¦è¡¨æ ¼ (ä½™å¼¦ç›¸ä¼¼åº¦)")
    print(f"{'='*60}")
    
    print("\n" + " " * 12, end="")
    for name in image_names:
        print(f"{name[:8]:>8}", end="")
    print()
    
    for i, name1 in enumerate(image_names):
        print(f"{name1[:8]:8}", end="")
        for j, name2 in enumerate(image_names):
            similarity = analyzer.cosine_similarity(
                features_dict[name1], 
                features_dict[name2]
            )
            print(f"{similarity:8.3f}", end="")
        print()
    
    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    print(f"\n{'='*60}")
    print("ğŸ“Š åˆ†ææŠ¥å‘Š")
    print(f"{'='*60}")
    
    print(f"\næ¨¡å‹ä¿¡æ¯:")
    model_info = extractor.get_model_info()
    for key, value in model_info.items():
        print(f"  {key}: {value}")
    
    # ç¼“å­˜ä¿¡æ¯
    cached_models = cache_manager.get_cached_models()
    print(f"\nç¼“å­˜ä¿¡æ¯:")
    print(f"  å·²ç¼“å­˜æ¨¡å‹: {', '.join(cached_models) if cached_models else 'æ— '}")
    print(f"  ç¼“å­˜ç›®å½•: {cache_manager.cache_dir.absolute()}")
    
    print(f"\næ–‡ä»¶è¾“å‡º:")
    print(f"  - {features_file}: ç‰¹å¾å‘é‡")
    for method in methods:
        matrix_file = os.path.join(output_dir, f'similarity_matrix_{method}_{model_name}.npy')
        if os.path.exists(matrix_file):
            print(f"  - {matrix_file}: ç›¸ä¼¼åº¦çŸ©é˜µ ({method})")
    
    viz_files = [
        f'similarity_heatmap_{model_name}.png',
        f'feature_scatter_{model_name}.png',
        f'tsne_visualization_{model_name}.png',
    ]
    
    for viz_file in viz_files:
        viz_path = os.path.join(output_dir, viz_file)
        if os.path.exists(viz_path):
            print(f"  - {viz_path}: å¯è§†åŒ–å›¾è¡¨")
    
    print(f"\n{'='*60}")
    print("ğŸ‰ åˆ†æå®Œæˆï¼")
    print(f"{'='*60}")
    
    # æ¸…ç†æç¤º
    print(f"\nğŸ’¡ æç¤º:")
    print(f"  ä¸‹æ¬¡è¿è¡Œå¯ä»¥ä½¿ç”¨å·²ç¼“å­˜çš„æ¨¡å‹ï¼Œæ— éœ€é‡æ–°ä¸‹è½½")
    print(f"  å¦‚éœ€æ¸…ç†ç¼“å­˜: python {sys.argv[0]} --clear-cache")

if __name__ == "__main__":
    # å¤„ç†å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1 and sys.argv[1] == "--clear-cache":
        cache_manager = ModelCacheManager()
        cache_manager.clear_cache()
        print("ç¼“å­˜å·²æ¸…ç†")
    else:
        main()